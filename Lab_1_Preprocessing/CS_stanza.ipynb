{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc682fd27cbf4cb7b67dda133514364b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 08:59:22 INFO: Downloading default packages for language: en (English)...\n",
      "2024-03-15 08:59:27 INFO: File exists: /home/claire/stanza_resources/en/default.zip.\n",
      "2024-03-15 08:59:35 INFO: Finished downloading models and saved to /home/claire/stanza_resources.\n",
      "2024-03-15 08:59:35 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2024-03-15 08:59:35 INFO: Use device: gpu\n",
      "2024-03-15 08:59:35 INFO: Loading: tokenize\n",
      "2024-03-15 08:59:51 INFO: Loading: pos\n",
      "2024-03-15 08:59:51 INFO: Loading: lemma\n",
      "2024-03-15 08:59:51 INFO: Loading: depparse\n",
      "2024-03-15 08:59:51 INFO: Loading: sentiment\n",
      "2024-03-15 08:59:51 INFO: Loading: constituency\n",
      "2024-03-15 08:59:52 INFO: Loading: ner\n",
      "2024-03-15 08:59:52 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download Stanza models\n",
    "stanza.download('en')\n",
    "\n",
    "# Initialize the default English pipeline\n",
    "nlp = stanza.Pipeline(lang=\"en\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is a sentence. This is another sentence'\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1  =======\n",
      "This is a sentence.\n",
      "====== Sentence 2  =======\n",
      "This is another sentence\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1}  =======')\n",
    "    print(sentence.text, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "0\n",
      "5\n",
      "8\n",
      "10\n",
      "18\n",
      "====== Sentence 2 tokens =======\n",
      "20\n",
      "25\n",
      "28\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "# start_char is the offset of each token in the input document\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'{word.start_char}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "This\n",
      "is\n",
      "a\n",
      "test\n",
      "sentence\n",
      "for\n",
      "stanza\n",
      ".\n",
      "====== Sentence 2 tokens =======\n",
      "This\n",
      "is\n",
      "another\n",
      "sentence\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Apply pipeline\n",
    "sz_doc = nlp('This is a test sentence for stanza. This is another sentence.')\n",
    "# Get the sentences (Sentence segmentation)\n",
    "for i, sentence in enumerate(sz_doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "# Get tokens using the \"words\" attribute (Tokenization)\n",
    "    print(*[f'{word.text}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "This\t\t this\n",
      "is\t\t be\n",
      "a\t\t a\n",
      "sentence\t\t sentence\n",
      ".\t\t .\n",
      "====== Sentence 2 tokens =======\n",
      "This\t\t this\n",
      "is\t\t be\n",
      "another\t\t another\n",
      "sentence\t\t sentence\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'{word.text}\\t\\t {word.lemma}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "This\t\t Number=Sing|PronType=Dem\n",
      "is\t\t Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "a\t\t Definite=Ind|PronType=Art\n",
      "sentence\t\t Number=Sing\n",
      ".\t\t None\n",
      "====== Sentence 2 tokens =======\n",
      "This\t\t Number=Sing|PronType=Dem\n",
      "is\t\t Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "another\t\t None\n",
      "sentence\t\t Number=Sing\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'{word.text}\\t\\t {word.feats}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "This\t\t nsubj\n",
      "is\t\t cop\n",
      "a\t\t det\n",
      "sentence\t\t root\n",
      ".\t\t punct\n",
      "====== Sentence 2 tokens =======\n",
      "This\t\t nsubj\n",
      "is\t\t cop\n",
      "another\t\t det\n",
      "sentence\t\t root\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'{word.text}\\t\\t {word.deprel}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constituency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "(ROOT (S (NP (DT This)) (VP (VBZ is) (NP (DT another) (NN sentence)))))\n",
      "====== Sentence 2 tokens =======\n",
      "(ROOT (S (NP (DT This)) (VP (VBZ is) (NP (DT a) (NN sentence))) (. .)))\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Simone de Beauvoir\ttype: PERSON\n",
      "entity: Paris\ttype: GPE\n",
      "entity: France\ttype: GPE\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "doc = nlp(\"Simone de Beauvoir was born in Paris. She lived in France.\")\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"This\",\n",
      "      \"lemma\": \"this\",\n",
      "      \"upos\": \"PRON\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Number=Sing|PronType=Dem\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 0,\n",
      "      \"end_char\": 4,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"is\",\n",
      "      \"lemma\": \"be\",\n",
      "      \"upos\": \"AUX\",\n",
      "      \"xpos\": \"VBZ\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"cop\",\n",
      "      \"start_char\": 5,\n",
      "      \"end_char\": 7,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \"a\",\n",
      "      \"lemma\": \"a\",\n",
      "      \"upos\": \"DET\",\n",
      "      \"xpos\": \"DT\",\n",
      "      \"feats\": \"Definite=Ind|PronType=Art\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"det\",\n",
      "      \"start_char\": 8,\n",
      "      \"end_char\": 9,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 4,\n",
      "      \"text\": \"test\",\n",
      "      \"lemma\": \"test\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"compound\",\n",
      "      \"start_char\": 10,\n",
      "      \"end_char\": 14,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 5,\n",
      "      \"text\": \"document\",\n",
      "      \"lemma\": \"document\",\n",
      "      \"upos\": \"NOUN\",\n",
      "      \"xpos\": \"NN\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 15,\n",
      "      \"end_char\": 23,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 6,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \".\",\n",
      "      \"head\": 5,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 23,\n",
      "      \"end_char\": 24,\n",
      "      \"ner\": \"O\"\n",
      "    }\n",
      "  ],\n",
      "  [\n",
      "    {\n",
      "      \"id\": 1,\n",
      "      \"text\": \"John\",\n",
      "      \"lemma\": \"John\",\n",
      "      \"upos\": \"PROPN\",\n",
      "      \"xpos\": \"NNP\",\n",
      "      \"feats\": \"Number=Sing\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"nsubj\",\n",
      "      \"start_char\": 25,\n",
      "      \"end_char\": 29,\n",
      "      \"ner\": \"S-PERSON\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 2,\n",
      "      \"text\": \"sleeps\",\n",
      "      \"lemma\": \"sleep\",\n",
      "      \"upos\": \"VERB\",\n",
      "      \"xpos\": \"VBZ\",\n",
      "      \"feats\": \"Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\",\n",
      "      \"head\": 0,\n",
      "      \"deprel\": \"root\",\n",
      "      \"start_char\": 30,\n",
      "      \"end_char\": 36,\n",
      "      \"ner\": \"O\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": 3,\n",
      "      \"text\": \".\",\n",
      "      \"lemma\": \".\",\n",
      "      \"upos\": \"PUNCT\",\n",
      "      \"xpos\": \".\",\n",
      "      \"head\": 2,\n",
      "      \"deprel\": \"punct\",\n",
      "      \"start_char\": 36,\n",
      "      \"end_char\": 37,\n",
      "      \"ner\": \"O\"\n",
      "    }\n",
      "  ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Documents to process\n",
    "documents = [\"This is a test document. John sleeps.\", \"I wrote another document for fun. Mary dreams.\"] \n",
    "# Wrap each document with a stanza.Document object\n",
    "in_docs = [stanza.Document([], text=d) for d in documents] \n",
    "# Call the neural pipeline on this list of documents\n",
    "docs = nlp(in_docs) \n",
    "\n",
    "# Look at the annotations produced for the first sentence by Stanza\n",
    "print(docs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
