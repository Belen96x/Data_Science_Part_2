{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises Lecture 13: Topic Modeling (Gensim)\n",
    "\n",
    "In this notebook, we use LDA to perform topic modeling on a corpus of Wikipedia articles from 16 categories:\n",
    "\n",
    "Airports, Artists, Astronauts, Astronomical_objects, Building,City,Comics_characters, Companies,Foods, Monuments_and_memorials,Politicians,Sports_teams,Sportspeople, Transport, Universities_and_colleges, Written_communication..\n",
    "\n",
    "The assignment involves the following steps:\n",
    "\n",
    "* Preparing the data  \n",
    "* Training an LDA model\n",
    "* Interpreting the results of the LDA model\n",
    "   - Printing out the topk relevant tokens for each topic\n",
    "   - Computing coherence\n",
    "   - Visualising the topic graph\n",
    "\n",
    "Data: wkp_sorted.zip      \n",
    "\n",
    "Python libraries\n",
    "- sklearn.datasets to load data \n",
    "- pandas\n",
    "- WordCloud\n",
    "- gensim for topic modeling  \n",
    "\n",
    "Cheat sheets\n",
    "- clustering_cheat_sheet.ipynb   \n",
    "- topic_modeling_cheat_sheet.ipynb\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Installing dependent packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyLDAvis in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (3.4.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from PyLDAvis) (1.24.2)\n",
      "Requirement already satisfied: scipy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from PyLDAvis) (1.10.1)\n",
      "Requirement already satisfied: pandas>=1.3.4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from PyLDAvis) (1.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from PyLDAvis) (1.2.0)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from PyLDAvis) (3.1.2)\n",
      "Requirement already satisfied: numexpr in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from PyLDAvis) (2.8.4)\n",
      "Requirement already satisfied: funcy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from PyLDAvis) (2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from PyLDAvis) (1.2.2)\n",
      "Requirement already satisfied: gensim in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from PyLDAvis) (4.3.1)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from PyLDAvis) (69.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas>=1.3.4->PyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas>=1.3.4->PyLDAvis) (2023.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from scikit-learn>=1.0.0->PyLDAvis) (3.1.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from gensim->PyLDAvis) (6.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from jinja2->PyLDAvis) (2.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=1.3.4->PyLDAvis) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# To be run only once\n",
    "#!pip install gensim\n",
    "#!pip install PyLDAvis\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating a word cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1** \n",
    "\n",
    "* Create a pandas dataframe containing a column for the text of each Wikipedia article included in  \"wkp_sorted/\". \n",
    "* Use sklearn.datasets load_files method (cf. clustering CS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Memphis Air Route Traffic Control Center (ZME)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Al-Haj Suliman Yari (19 August 1936 – 10 May 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Sapphire Stagg is a fictional character appear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Relocation of professional sports teams occurs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>In aerodynamics, pitch-up is an uncommanded no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text\n",
       "155  Memphis Air Route Traffic Control Center (ZME)...\n",
       "156  Al-Haj Suliman Yari (19 August 1936 – 10 May 2...\n",
       "157  Sapphire Stagg is a fictional character appear...\n",
       "158  Relocation of professional sports teams occurs...\n",
       "159  In aerodynamics, pitch-up is an uncommanded no..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_files\n",
    "# Loading all files in \"dir\" directory into a pandas dataframe\n",
    "DATA_DIR = \"arche_topic_models/wkp_sorted/\"\n",
    "data = load_files(DATA_DIR, encoding=\"utf-8\", decode_error=\"replace\")\n",
    "df = pd.DataFrame({'text': data['data']})\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Generate a word cloud (topic_modeling CS)\n",
    "\n",
    "* The WordCloud method takes as input the corpus as a single string. \n",
    "* Use pandas str.cat method to concatenate the content of the \"story_str\" column into a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1061304f0>: Failed to establish a new connection: [Errno 61] Connection refused')': /simple/wordcloud/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x106130fd0>: Failed to establish a new connection: [Errno 61] Connection refused')': /simple/wordcloud/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x106153280>: Failed to establish a new connection: [Errno 61] Connection refused')': /simple/wordcloud/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x106153430>: Failed to establish a new connection: [Errno 61] Connection refused')': /simple/wordcloud/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x1061535e0>: Failed to establish a new connection: [Errno 61] Connection refused')': /simple/wordcloud/\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement WordCloud (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for WordCloud\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import the wordcloud library\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Join the synopses into a single string.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#long_string = movies.str.cat(sep=\" \")\u001b[39;00m\n\u001b[1;32m      6\u001b[0m long_string \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mcat(sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "# Join the synopses into a single string.\n",
    "#long_string = movies.str.cat(sep=\" \")\n",
    "\n",
    "long_string = df['text'].str.cat(sep=\" \")\n",
    "# Create a WordCloud objecta\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=1000, contour_width=3, contour_color='steelblue')\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3:** Preparing the corpus for topic modeling\n",
    "\n",
    "Gensim topic modeling module takes as input a list of tokens.\n",
    "\n",
    " - Define a clean_up function which takes as input a list of texts and outputs the list of lemmas for tokens in the input which :\n",
    "* are not stop words  (spacy CS)\n",
    "* only contains characters (python_basic CS) \n",
    "* whose length is greater than 2\n",
    "* whose spacy POS tag is not 'ADV','PRON','CCONJ','PUNCT','PART','DET','ADP'or 'SPACE'  (spacy CS)\n",
    "\n",
    "- Apply this function to the 'text' column of the Wikipedia dataframe (cf. Ex. 1 and 2)   \n",
    "_**Help**_ : use pandas apply method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stanza'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstanza\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Download Stanza models\u001b[39;00m\n\u001b[1;32m      4\u001b[0m stanza\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stanza'"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download Stanza models\n",
    "stanza.download('en')\n",
    "\n",
    "# Initialize the default English pipeline\n",
    "nlp = stanza.Pipeline(lang=\"en\", processors='tokenize,pos,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 61]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>no_headers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Cardiff Roller Collective (CRoC) are a rol...</td>\n",
       "      <td>The Cardiff Roller Collective (CRoC) are a rol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Go! Pack Go!\" is the fight song of the Green ...</td>\n",
       "      <td>\"Go! Pack Go!\" is the fight song of the Green ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Al-Machriq (English translation: The East) was...</td>\n",
       "      <td>Al-Machriq (English translation: The East) was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ajman International Airport (Arabic: مطار عجما...</td>\n",
       "      <td>Ajman International Airport (Arabic: مطار عجما...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kapla is a construction set for children and a...</td>\n",
       "      <td>Kapla is a construction set for children and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>Memphis Air Route Traffic Control Center (ZME)...</td>\n",
       "      <td>Memphis Air Route Traffic Control Center (ZME)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>Al-Haj Suliman Yari (19 August 1936 – 10 May 2...</td>\n",
       "      <td>Al-Haj Suliman Yari (19 August 1936 – 10 May 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>Sapphire Stagg is a fictional character appear...</td>\n",
       "      <td>Sapphire Stagg is a fictional character appear...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>Relocation of professional sports teams occurs...</td>\n",
       "      <td>Relocation of professional sports teams occurs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>In aerodynamics, pitch-up is an uncommanded no...</td>\n",
       "      <td>In aerodynamics, pitch-up is an uncommanded no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>160 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  \\\n",
       "0    The Cardiff Roller Collective (CRoC) are a rol...   \n",
       "1    \"Go! Pack Go!\" is the fight song of the Green ...   \n",
       "2    Al-Machriq (English translation: The East) was...   \n",
       "3    Ajman International Airport (Arabic: مطار عجما...   \n",
       "4    Kapla is a construction set for children and a...   \n",
       "..                                                 ...   \n",
       "155  Memphis Air Route Traffic Control Center (ZME)...   \n",
       "156  Al-Haj Suliman Yari (19 August 1936 – 10 May 2...   \n",
       "157  Sapphire Stagg is a fictional character appear...   \n",
       "158  Relocation of professional sports teams occurs...   \n",
       "159  In aerodynamics, pitch-up is an uncommanded no...   \n",
       "\n",
       "                                            no_headers  \n",
       "0    The Cardiff Roller Collective (CRoC) are a rol...  \n",
       "1    \"Go! Pack Go!\" is the fight song of the Green ...  \n",
       "2    Al-Machriq (English translation: The East) was...  \n",
       "3    Ajman International Airport (Arabic: مطار عجما...  \n",
       "4    Kapla is a construction set for children and a...  \n",
       "..                                                 ...  \n",
       "155  Memphis Air Route Traffic Control Center (ZME)...  \n",
       "156  Al-Haj Suliman Yari (19 August 1936 – 10 May 2...  \n",
       "157  Sapphire Stagg is a fictional character appear...  \n",
       "158  Relocation of professional sports teams occurs...  \n",
       "159  In aerodynamics, pitch-up is an uncommanded no...  \n",
       "\n",
       "[160 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def rm_headers(text):\n",
    "    return re.sub('=+ (\\w+) =+', \"\", text)\n",
    "\n",
    "no_headers = df[\"text\"].apply(rm_headers)\n",
    "\n",
    "X = pd.Series(no_headers,name=\"no_headers\")\n",
    "df = pd.concat([df,X],axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models,corpora\n",
    "\n",
    "# Step-1: clean up your text and generate list of words for each document. \n",
    "def clean_up(text):\n",
    "    removal=['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
    "    text_out = []\n",
    "    doc= nlp(text)\n",
    "    tokens = [(tok.text,tok.upos,tok.lemma) for stce in doc.sentences for tok in stce.words]\n",
    "    #print(list(zip(*tokens))[0])\n",
    "    for (token,upos,lemma) in tokens:\n",
    "        if  token.isalpha() and len(token)>2 and upos not in removal  and token not in stopwords.words(\"english\"):\n",
    "            text_out.append(lemma)\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models,corpora\n",
    "\n",
    "# Step-1: clean up your text and generate list of words for each document. \n",
    "def clean_up(text):\n",
    "    removal=['ADV','PRON','CCONJ','PUNCT','PART','DET','ADP','SPACE']\n",
    "    text_out = []\n",
    "    doc= nlp(text)\n",
    "    tokens = [(tok.text,tok.upos,tok.lemma) for stce in doc.sentences for tok in stce.words]\n",
    "    #print(list(zip(*tokens))[0])\n",
    "    for (token,upos,lemma) in tokens:\n",
    "        if  token.isalpha() and len(token)>2:\n",
    "            if upos  in removal  and token  in stopwords.words(\"english\"):\n",
    "                continue\n",
    "            text_out.append(lemma)\n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    Kapla is a construction set for children and a...\n",
       "5    Akmal Ikramovich Ikramov (Russia: Акмаль Икрам...\n",
       "Name: no_headers, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['no_headers'][4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [clean_up(x) for x in df['no_headers'][0:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for x in df['no_headers']:\n",
    "    counter += 1\n",
    "    print(counter)\n",
    "      #clean_up(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmas\n",
    "#lemmas_list = [s.split(\" \") for s in lemmas]\n",
    "lemmas_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = pd.Series(lemmas,name=\"out_text\")\n",
    "df = pd.concat([df['text'],X],axis = 1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store in  a csv file\n",
    "df[\"out_text\"].to_csv('clean_wkp.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn a topic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4:** Create a vocabulary for the lda model and convert your list of list of lemmas into a document-term matrix\n",
    "\n",
    "* Use [Gensim dictionary method](https://radimrehurek.com/gensim/corpora/dictionary.html) to create a dictionary \n",
    "* Use Gensim doc2bow method (from Corpora module) to convert each synopsis to a list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step-2: Create a vocabulary for the lda model and also convert our corpus into document-term matrix for Lda\n",
    "# # the input must be a list of lists of tokens\n",
    "# # Create the word2id dictionary which maps each token to an integer\n",
    "dictionary = corpora.Dictionary(lemmas_list)\n",
    "# each doc corresponds to an input file\n",
    "# the input must be a list of lists of tokens\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in lemmas_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5:** Create an LDA model with 16 topics and apply it to your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics =16\n",
    "Lda = models.LdaMulticore\n",
    "lda_model= Lda(doc_term_matrix, \n",
    "               num_topics=num_topics,\n",
    "               id2word = dictionary, \n",
    "               passes=20,chunksize=2000,random_state=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 6:** Print out the keywords of the 16 topics (Airports, Artists, Astronauts, Astronomical_objects, Building,City,Comics_characters, Companies,Foods, Monuments_and_memorials,Politicians,Sports_teams,Sportspeople, Transport, Universities_and_colleges, Written_communication)\n",
    "\n",
    "\n",
    "Each topic is a combination of keywords.\n",
    "\n",
    "* Use `lda_model.print_topics()` to see the keywords for each topic and the weight of each keyword for that topic\n",
    "* Retrain you LDA model with different numbers of topics and examine the top keywords to determine which number of topics is best\n",
    "* Can you match the topics to the Wikipedia categories ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Print the Keyword in the 16 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[doc_term_matrix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 7:** Compute Model Perplexity and Coherence Score\n",
    "\n",
    "* A lower perplexity score indicates better generalization performance\n",
    "* Coherence measures score a  topic by measuring the degree of semantic similarity between high scoring words in the topic.\n",
    "\n",
    "1. `C_v` measure is based on a sliding window, one-set segmentation of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarity\n",
    "2. `C_p` is based on a sliding window, one-preceding segmentation of the top words and the confirmation measure of Fitelson's coherence\n",
    "3. `C_uci` measure is based on a sliding window and the pointwise mutual information (PMI) of all word pairs of the given top words\n",
    "4. `C_umass` is based on document cooccurrence counts, a one-preceding segmentation and a logarithmic conditional probability as confirmation measure\n",
    "5. `C_npmi` is an enhanced version of the C_uci coherence using the normalized pointwise mutual information (NPMI)\n",
    "6. `C_a` is based on a context window, a pairwise comparison of the top words and an indirect confirmation measure that uses normalized pointwise mutual information (NPMI) and the cosine similarit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('Perplexity:', lda_model.log_perplexity(doc_term_matrix))\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=lemmas_list, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the topic model using pyLDAvis (PROVIDED)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter tuning (PROVIDED)\n",
    "\n",
    "First, let's differentiate between model hyperparameters and model parameters :\n",
    "\n",
    "- `Model hyperparameters` can be thought of as settings for a machine learning algorithm that are tuned by the data scientist before training. Examples would be the number of trees in the random forest, or in our case, number of topics K\n",
    "\n",
    "- `Model parameters` can be thought of as what the model learns during training, such as the weights for each word in a given topic.\n",
    "\n",
    "Now that we have the baseline coherence score for the default LDA model, let's perform a series of sensitivity tests to help determine the following model hyperparameters: \n",
    "- Number of Topics (K)\n",
    "- Dirichlet hyperparameter alpha: Document-Topic Density\n",
    "- Dirichlet hyperparameter beta: Word-Topic Density\n",
    "\n",
    "We'll perform these tests in sequence, one parameter at a time by keeping others constant and run them over the two difference validation corpus sets. We'll use `C_v` as our choice of metric for performance comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supporting function\n",
    "def compute_coherence_values(corpus, dictionary, k, a, b):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=doc_term_matrix,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=a,\n",
    "                                           eta=b)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=doc_term_matrix, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call the function, and iterate it over the range of topics, alpha, and beta parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import gensim\n",
    "\n",
    "corpus = doc_term_matrix\n",
    "grid = {}\n",
    "grid['Validation_Set'] = {}\n",
    "\n",
    "# Topics range\n",
    "min_topics = 2\n",
    "max_topics = 11\n",
    "step_size = 1\n",
    "topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# Alpha parameter\n",
    "alpha = list(np.arange(0.01, 1, 0.3))\n",
    "alpha.append('symmetric')\n",
    "alpha.append('asymmetric')\n",
    "\n",
    "# Beta parameter\n",
    "beta = list(np.arange(0.01, 1, 0.3))\n",
    "beta.append('symmetric')\n",
    "\n",
    "# Validation sets\n",
    "num_of_docs = len(corpus)\n",
    "corpus_sets = [corpus]\n",
    "\n",
    "corpus_title = ['100% Corpus']\n",
    "\n",
    "model_results = {'Validation_Set': [],\n",
    "                 'Topics': [],\n",
    "                 'Alpha': [],\n",
    "                 'Beta': [],\n",
    "                 'Coherence': []\n",
    "                }\n",
    "\n",
    "# Can take a long time to run\n",
    "if 1 == 1:\n",
    "    pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)*len(corpus_title)))\n",
    "    \n",
    "    # iterate through validation corpuses\n",
    "    for i in range(len(corpus_sets)):\n",
    "        # iterate through number of topics\n",
    "        for k in topics_range:\n",
    "            # iterate through alpha values\n",
    "            for a in alpha:\n",
    "                # iterare through beta values\n",
    "                for b in beta:\n",
    "                    # get the coherence score for the given parameters\n",
    "                    cv = compute_coherence_values(corpus=corpus_sets[i], dictionary=dictionary, \n",
    "                                                  k=k, a=a, b=b)\n",
    "                    # Save the model results\n",
    "                    model_results['Validation_Set'].append(corpus_title[i])\n",
    "                    model_results['Topics'].append(k)\n",
    "                    model_results['Alpha'].append(a)\n",
    "                    model_results['Beta'].append(b)\n",
    "                    model_results['Coherence'].append(cv)\n",
    "                    \n",
    "                    pbar.update(1)\n",
    "    pd.DataFrame(model_results).to_csv('lda_tuning_results.csv', index=False)\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model Training\n",
    "\n",
    "Based on external evaluation (Code to be added from Excel based analysis), train the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=16, \n",
    "                                           random_state=100,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha=0.01,\n",
    "                                           eta=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# Compute Perplexity\n",
    "print('Perplexity:', lda_model.log_perplexity(doc_term_matrix))\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=lemmas_list, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('Coherence Score: ', coherence_lda)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aleenv",
   "language": "python",
   "name": "aleenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
