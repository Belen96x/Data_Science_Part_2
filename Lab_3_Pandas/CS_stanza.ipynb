{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25d5426189ee42e38e749a71d1a0c8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.3.0.json:   0%|   â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-30 19:54:45 INFO: Downloading default packages for language: en (English)...\n",
      "2023-01-30 19:54:56 INFO: File exists: /home/claire/stanza_resources/en/default.zip.\n",
      "2023-01-30 19:55:04 INFO: Finished downloading models and saved to /home/claire/stanza_resources.\n",
      "2023-01-30 19:55:04 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| constituency | wsj       |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-01-30 19:55:04 INFO: Use device: gpu\n",
      "2023-01-30 19:55:04 INFO: Loading: tokenize\n",
      "2023-01-30 19:56:32 INFO: Loading: pos\n",
      "2023-01-30 19:56:32 INFO: Loading: lemma\n",
      "2023-01-30 19:56:33 INFO: Loading: depparse\n",
      "2023-01-30 19:56:33 INFO: Loading: sentiment\n",
      "2023-01-30 19:56:33 INFO: Loading: constituency\n",
      "2023-01-30 19:56:33 INFO: Loading: ner\n",
      "2023-01-30 19:56:34 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# Download Stanza models\n",
    "stanza.download('en')\n",
    "\n",
    "# Initialize the default English pipeline\n",
    "nlp = stanza.Pipeline(lang=\"en\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is a sentence. This is another sentence'\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1  =======\n",
      "This is a sentences.\n",
      "====== Sentence 2  =======\n",
      "This is another sentence\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1}  =======')\n",
    "    print(sentence.text, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "0\n",
      "5\n",
      "8\n",
      "10\n",
      "18\n",
      "====== Sentence 2 tokens =======\n",
      "20\n",
      "25\n",
      "28\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "# start_char is the offset of each token in the input document\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'{word.start_char}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "This\n",
      "is\n",
      "a\n",
      "test\n",
      "sentence\n",
      "for\n",
      "stanza\n",
      ".\n",
      "====== Sentence 2 tokens =======\n",
      "This\n",
      "is\n",
      "another\n",
      "sentence\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# Apply pipeline\n",
    "sz_doc = nlp('This is a test sentence for stanza. This is another sentence.')\n",
    "# Get the sentences (Sentence segmentation)\n",
    "for i, sentence in enumerate(sz_doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "# Get tokens using the \"words\" attribute (Tokenization)\n",
    "    print(*[f'{word.text}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "This\t\t this\n",
      "is\t\t be\n",
      "a\t\t a\n",
      "test\t\t test\n",
      "sentence\t\t sentence\n",
      "for\t\t for\n",
      "stanza\t\t stanza\n",
      ".\t\t .\n",
      "====== Sentence 2 tokens =======\n",
      "This\t\t this\n",
      "is\t\t be\n",
      "another\t\t another\n",
      "sentence\t\t sentence\n",
      ".\t\t .\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'{word.text}\\t\\t {word.lemma}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Morphological Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "This\t\t Number=Sing|PronType=Dem\n",
      "is\t\t Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "a\t\t Definite=Ind|PronType=Art\n",
      "test\t\t Number=Sing\n",
      "sentence\t\t Number=Sing\n",
      "for\t\t None\n",
      "stanza\t\t Number=Sing\n",
      ".\t\t None\n",
      "====== Sentence 2 tokens =======\n",
      "This\t\t Number=Sing|PronType=Dem\n",
      "is\t\t Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
      "another\t\t None\n",
      "sentence\t\t Number=Sing\n",
      ".\t\t None\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'{word.text}\\t\\t {word.feats}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "This\t\t nsubj\n",
      "is\t\t cop\n",
      "a\t\t det\n",
      "test\t\t compound\n",
      "sentence\t\t root\n",
      "for\t\t case\n",
      "stanza\t\t nmod\n",
      ".\t\t punct\n",
      "====== Sentence 2 tokens =======\n",
      "This\t\t nsubj\n",
      "is\t\t cop\n",
      "another\t\t det\n",
      "sentence\t\t root\n",
      ".\t\t punct\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(*[f'{word.text}\\t\\t {word.deprel}' for word in sentence.words], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constituency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====== Sentence 1 tokens =======\n",
      "(ROOT (S (NP (DT This)) (VP (VBZ is) (NP (DT another) (NN sentence))) (. .)))\n",
      "====== Sentence 2 tokens =======\n",
      "(ROOT (S (NP (DT This)) (VP (VBZ is) (NP (NP (DT a) (NN test) (NN sentence)) (PP (IN for) (NP (NN stanza))))) (. .)))\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "for i, sentence in enumerate(doc.sentences):\n",
    "    print(f'====== Sentence {i+1} tokens =======')\n",
    "    print(sentence.constituency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Simone de Beauvoir\ttype: PERSON\n",
      "entity: Paris\ttype: GPE\n",
      "entity: France\ttype: GPE\n"
     ]
    }
   ],
   "source": [
    "# doc is the text above together with its Stanza created annotations\n",
    "doc = nlp(\"Simone de Beauvoir was born in Paris. She lived in France.\")\n",
    "print(*[f'entity: {ent.text}\\ttype: {ent.type}' for ent in doc.ents], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing multiple documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents to process\n",
    "documents = [\"This is a test document. John sleeps.\", \"I wrote another document for fun. Mary dreams.\"] \n",
    "# Wrap each document with a stanza.Document object\n",
    "in_docs = [stanza.Document([], text=d) for d in documents] \n",
    "# Call the neural pipeline on this list of documents\n",
    "docs = nlp(in_docs) \n",
    "\n",
    "# Look at the annotations produced for the first sentence by Stanza\n",
    "print(docs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
