{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Lecture 8: The Spacy Pipeline\n",
    "\n",
    "In this session, we take a closer look at Spacy. \n",
    "\n",
    "* What happens when we apply a model to a text ?\n",
    "* Spacy pipelines\n",
    "* Spacy attributes\n",
    "* Word and sentence similarity\n",
    "* Accessing and creating spans\n",
    "* Adding components to a Spacy pipeline\n",
    "* Adding attributes\n",
    "\n",
    "[Spacy Cheat-sheet](https://www.datacamp.com/cheat-sheet/spacy-cheat-sheet-advanced-nlp-in-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a language model\n",
    "We are going to load the English language model. For other languages see: https://spacy.io/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Spacy model \"en_core_web_sm\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Apply the model to a text\n",
    "doc = nlp(\"This is a text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Models/Pipelines\n",
    "\n",
    "#### What does the nlp object actually do?\n",
    "\n",
    "- Spacy models are _**pipelines**_ made of several components  e.g., a tokenizer and a tagger\n",
    "- Applying a Spacy model (here the \"nlp\" object) to a string yields a _**Doc object**_ that contains various types of linguistic annotations/attributes for _**text, tokens and spans**_ depending on which components the model is made of. \n",
    "- For instance, if the model contains a POS tagger, applying this model to a text will yield a doc object which associates each token in the text with a POS tag.\n",
    "- A _**pipeline component**_ is a function which takes a Doc object as input, modify it and return the modified doc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accessing the text of each token t\n",
    "[token.text for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Accessing the text of each token t\n",
    "[token for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type([token for token in doc][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type([token.text for token in doc][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting information about the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List the model's components\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Attributes\n",
    "- Spacy Attributes return label IDs. \n",
    "- For **string labels**, use the attributes with an underscore. For example, token.pos_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part-of-speech tags (predicted by statistical model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a text.\")\n",
    "# Coarse-grained part-of-speech tags\n",
    "[token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-grained part-of-speech tags\n",
    "[token.tag_ for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label explanations\n",
    "spacy.explain(\"RB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"RB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain(\"GPE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Syntactic dependencies (predicted by statistical model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency labels\n",
    "[(token.text, token.dep_)for token in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Syntactic head token (governor)\n",
    "[(token.text, token.head.text)for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named Entities (predicted by statistical model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Henry Poincaré was born in Nancy, France\")\n",
    "# Text and label of named entity span\n",
    "[(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentences (usually needs the dependency parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Henry Poincaré was born in Nancy. Henri Poincaré was a French genius who revolutionized mathematics, physics, and philosophy in the late 19th century.\")\n",
    "[sent.text for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NP Chunks\n",
    "- Base noun phrases (needs the tagger and parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I have a red car\")\n",
    "# doc.noun_chunks is a generator that yields spans\n",
    "[chunk.text for chunk in doc.noun_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Henri Poincaré was a French genius who revolutionized mathematics, physics, and philosophy in the late 19th century.\")\n",
    "# doc.noun_chunks is a generator that yields spans\n",
    "[chunk.text for chunk in doc.noun_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting information about Spacy attributes\n",
    "Type 'doc.' followed by TAB to see the different methods and data structures. Take your time to check out the documentation at: https://spacy.io/api/ to learn what they are and what you can do. The better you understand the data objects and function the easier it will be to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.lang_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing\n",
    "⚠️ If you're in a Jupyter notebook, use displacy.render. Otherwise, use displacy.serve to start a web server and show the visualization in your browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"This is a sentence\")\n",
    "displacy.render(doc, style=\"dep\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize named entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Larry Page founded Google\")\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word and Sentence  similarity\n",
    "⚠️ To use word vectors, you need to install the larger models ending in md or lg , for example en_core_web_lg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I like dogs\")\n",
    "# Compare 2 documents\n",
    "doc1.similarity(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare 2 tokens\n",
    "doc1[2].similarity(doc2[2])\n",
    "# Compare tokens and spans\n",
    "doc1[0].similarity(doc2[1:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector as a numpy array\n",
    "doc = nlp(\"I like cats\")\n",
    "# The L2 norm of the token's vector\n",
    "doc[2].vector\n",
    "#doc[2].vector_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spans\n",
    "\n",
    "There are three basic units in Spacy: documents, tokens and spans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"I live in New York\")\n",
    "# Span for \"New York\" with label GPE (geopolitical)\n",
    "span = doc[3:6]\n",
    "span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Span object\n",
    "from spacy.tokens import Span\n",
    "# Create a Doc object\n",
    "doc = nlp(\"I live in X Y\")\n",
    "# Span for \"New York\" with label GPE (geopolitical)\n",
    "span = Span(doc, 3, 5, label=\"GPE\")\n",
    "span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "span.label_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom components: Modifying the Pipeline\n",
    "Components can be added to a pipeline first, last (default), or before or after an existing component.\n",
    "\n",
    "Custom pipeline components let you add your own component to the spaCy pipeline that is executed when you call the pipeline on a text – for example, to modify the doc and add more data to it.\n",
    "\n",
    "- A custom component is a function which modifies a doc and returns the modified version\n",
    "- Custom component are defined using the following syntax\n",
    "\n",
    "<code>@Language.component(\"custom_component\")\n",
    "def custom_component(doc):\n",
    "    Do something to the doc here \n",
    "    return doc\n",
    "</code>\n",
    "\n",
    "- Use the <code>nlp.add_pipe</code> method to add the component to the pipeline.\n",
    "- Remember to use the string name of the component \n",
    "\n",
    "<code>nlp.add_pipe(custom_component, first=True)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A custom component that prints the number of tokens in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"length_component\")\n",
    "def length_component_function(doc):\n",
    "    # Get the doc's length \n",
    "    doc_length = len(doc)\n",
    "    print(f\"This document is {doc_length} tokens long.\")\n",
    "    # Return the doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Load the small English pipeline\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Add the component first in the pipeline and print the pipe names\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process a text\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding custom attributes to the Doc, Token and Span objects \n",
    "- You can also add custom attribute to create and store additional annotations about the input text\n",
    "\n",
    "- declare new token attribute \"ATTR\"\n",
    "\n",
    "> <code>Token.set_extension(\"ATTR\", default=False)</code>\n",
    "\n",
    "- assign attribute to token\n",
    "\n",
    "> <code>doc[i]._.ATTR = True</code>\n",
    "\n",
    "- print attribute for input text\n",
    "\n",
    "> <code>print([token._.ATTR) for token in doc])</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding new attributes\n",
    "- Custom attributes can be added to the Doc, Token and Span objects.\n",
    "- Custom attributes  are registered on the global Doc, Token and Span classes and become available as <code>._.attr</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Token, Span\n",
    "doc = nlp(\"The sky over New York is blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute extensions (with default value)\n",
    "\n",
    "- Adding  the <code>is_country</code> attribute for tokens which denote a country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Token.set_extension to register \"is_country\" (default False).\n",
    "# Update it for \"Spain\" and print it for all tokens.\n",
    "\n",
    "import spacy\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Register the Token extension attribute \"is_country\" with the default value False\n",
    "Token.set_extension(\"is_country\", default=False)\n",
    "\n",
    "# Process the text and set the is_country attribute to True for the token \"Spain\"\n",
    "doc = nlp(\"I live in Spain.\")\n",
    "doc[3]._.is_country = True\n",
    "\n",
    "# Print the token text and the is_country attribute for all tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attribute extensions (with getter & setter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register custom attribute \"reversed\" on Doc class\n",
    "get_reversed = lambda doc: doc.text[::-1]\n",
    "Doc.set_extension(\"reversed\", getter=get_reversed)\n",
    "# Compute value of extension attribute with getter\n",
    "doc._.reversed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom attribute \"reversed\" on Token class\n",
    "\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Define the getter function that takes a token and returns its reversed text\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "# Register the Token property extension \"reversed\" with the getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed, force=True)\n",
    "\n",
    "# Process the text and print the reversed attribute for each token\n",
    "doc = nlp(\"All generalizations are false, including this one.\")\n",
    "for token in doc:\n",
    "    print(\"reversed:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Span extensions (with getter & setter)\n",
    "- Enriching named entities with their wikipedia URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_wikipedia_url(span):\n",
    "    # Get a Wikipedia URL if the span has one of the labels\n",
    "    if span.label_ in (\"PERSON\", \"ORG\", \"GPE\", \"LOCATION\"):\n",
    "        entity_text = span.text.replace(\" \", \"_\")\n",
    "        return \"https://en.wikipedia.org/w/index.php?search=\" + entity_text\n",
    "\n",
    "# Set the Span extension wikipedia_url using the getter get_wikipedia_url\n",
    "Span.set_extension(\"wikipedia_url\", getter=get_wikipedia_url)\n",
    "\n",
    "doc = nlp(\n",
    "    \"In over fifty years from his very first recordings right through to his \"\n",
    "    \"last album, David Bowie was at the vanguard of contemporary culture in Paris.\"\n",
    ")\n",
    "for ent in doc.ents:\n",
    "    # Print the text and Wikipedia URL of the entity\n",
    "    print(ent.text, ent._.wikipedia_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based matching\n",
    "\n",
    "spaCy’s rule-based Matcher lets you define rules to find words and phrases in text.\n",
    " https://spacy.io/api/matcher\n",
    " \n",
    " - Import the <code>Matcher</code> from <code>spacy.matcher</code>.\n",
    " - Initialize it with the nlp object’s shared vocab. The shared vocabulary is available as the <code>nlp.vocab</code> attribute.\n",
    " - Create a pattern \n",
    " - Use the <code>matcher.add</code> method to add the pattern to the matcher.\n",
    " - Call the matcher on the doc and store the result in the variable \"matches\".\n",
    " - Iterate over the matches and get the matched span from the start to the end index.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SpaCy Patterns\n",
    " \n",
    " A _**pattern**_ is a list of dictionaries keyed by the attribute names. For example,<code>[{\"TEXT\": \"Hello\"}]</code> will match one token whose exact text is “Hello”.\n",
    " \n",
    " The start and end values of each match describe the start and end index of the matched span. To get the span, you can create a slice of the doc using the given start and end.\n",
    "\n",
    " - To match a token with an exact text, you can use the TEXT attribute. For example, <code>{\"TEXT\": \"Apple\"}</code> will match tokens with the exact text “Apple”.\n",
    " - To match a number token, you can use the <code>\"IS_DIGIT\"</code> attribute, which will only return True for tokens consisting of only digits.\n",
    " - To specify a lemma, you can use the \"LEMMA\" attribute in the token pattern. For example, <code>{\"LEMMA\": \"be\"}</code> will match tokens like “is”, “was” or “being”.\n",
    " - To find proper nouns, you want to match all tokens whose \"POS\" value equals \"PROPN\": <code>{\"POS\": \"PROPN\"}</code>\n",
    " - Operators can be added via the \"OP\" key. For example, \"OP\": \"?\" to match zero or one time.\n",
    " - A pattern for adjective plus one or two nouns   \n",
    " <code>[{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\", \"OP\": \"?\"}]</code>\n",
    " \n",
    " See https://spacy.io/api/matcher for the full description of Spacy pattern format\n",
    " \n",
    " See https://course.spacy.io/en/chapter1 for some more pattern examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from spacy.matcher import Matcher\n",
    "# Matcher is initialized with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Pattern for New York, new york, new York or New york\n",
    "pattern = [{\"LOWER\": \"new\"}, {\"LOWER\": \"york\"}]\n",
    "\n",
    "# Add pattern to matcher\n",
    "matcher.add('CITIES', [pattern])\n",
    "\n",
    "# Match by calling the matcher on a Doc object\n",
    "doc = nlp(\"I live in New York\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Matches are (match_id, start, end) tuples\n",
    "for match_id, start, end in matches:\n",
    "     # Get the matched span by slicing the Doc\n",
    "     span = doc[start:end]\n",
    "     print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"love cats\", \"loving cats\", \"loved cats\"\n",
    "pattern1 = [{\"LEMMA\": \"love\"}, {\"LOWER\": \"cats\"}]\n",
    "# \"10 people\", \"twenty people\"\n",
    "pattern2 = [{\"LIKE_NUM\": True}, {\"TEXT\": \"people\"}]\n",
    "# \"book\", \"a cat\", \"the sea\" (noun + optional article)\n",
    "pattern3 = [{\"POS\": \"DET\", \"OP\": \"?\"}, {\"POS\": \"NOUN\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operators and quantifiers\n",
    "Can be added to a token dict as the \"OP\" key.\n",
    "\n",
    "OP\tDescription\n",
    "- !\tNegate pattern and match exactly 0 times.\n",
    "- ?\tMake pattern optional and match 0 or 1 times.\n",
    "- \\+\tRequire pattern to match 1 or more times.\n",
    "- \\*\tAllow pattern to match 0 or more times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching word forms and POS tags\n",
    "Search for forms of “download” (tokens with the lemma “download”), followed by a token with the part-of-speech tag \"PROPN\" (proper noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"i downloaded Fortnite on my laptop and can't open the game at all. Help? \"\n",
    "    \"so when I was downloading Minecraft, I got the Windows version where it \"\n",
    "    \"is the '.zip' folder and I used the default program to unpack it... do \"\n",
    "    \"I also need to download Winzip?\"\n",
    ")\n",
    "\n",
    "# Write a pattern that matches a form of \"download\" plus proper noun\n",
    "pattern = [{\"LEMMA\": \"download\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Add the pattern to the matcher and apply the matcher to the doc\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total matches found:\", len(matches))\n",
    "\n",
    "# Iterate over the matches and print the span text\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Match found:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Creating a pattern from a list of tokens\n",
    "- The **Matcher** lets you match sequences based on lists of token descriptions, \n",
    "- The **PhraseMatcher** lets you efficiently match large terminology lists. \n",
    "- The PhraseMatcher accepts match patterns in the form of Doc objects.\n",
    " \n",
    "https://spacy.io/api/phrasematcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "COUNTRIES = [\"France\",\"Germany\",\"Czech Republic\",\"Slovakia\"]\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "\n",
    "# Import the PhraseMatcher and initialize it\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Create pattern Doc objects and add them to the matcher\n",
    "# This is the faster version of: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Call the matcher on the test document and print the result\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a custom component which adds a new attribute using Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  The custom component adds  the \"capital\" attributes for tokens which denote a country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "\n",
    "COUNTRIES = [\"France\",\"Germany\",\"Czech Republic\",\"Slovakia\"]\n",
    "\n",
    "# Empty pipeline for English (tokenizer)\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component_function(doc):\n",
    "    # Get al matches (all countries)\n",
    "    matches = matcher(doc)\n",
    "    # Create an entity Span with the label \"GPE\" for each match\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "\n",
    "CAPITALS = {'Afghanistan': 'Kabul', 'Czech Republic': 'Prague','Slovakia': 'Bratislava', 'Slovenia': 'Ljubljana'}\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "# retrieves capital from dictionary using python get method for dictionaries\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to the doc.ents. \n",
    "\n",
    "A PhraseMatcher with the animal patterns has already been created as the variable matcher.\n",
    "\n",
    "- Define the custom component and apply the matcher to the doc.\n",
    "- Create a Span for each match, assign the label ID for \"ANIMAL\" and overwrite the doc.ents with the new spans.\n",
    "- Add the new component to the pipeline after the \"ner\" component.\n",
    "- Process the text and print the entity text and entity label for the entities in doc.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "\n",
    "# Pattern that matches any animal in the animals list\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"animal_patterns:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", animal_patterns)\n",
    "\n",
    "# Define the custom component\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Apply the matcher to the doc\n",
    "    matches = matcher(doc)\n",
    "    # Create a Span for each match and assign the label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Overwrite the doc.ents with the matched spans\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline after the \"ner\" component\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling\n",
    "Instead of iterating over the texts and processing them, iterate over the doc objects yielded by nlp.pipe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "with open(\"../data/webnlg-test.txt\") as infile:\n",
    "    novel = infile.readlines()\n",
    "    #print(file_content[:50])\n",
    "# Process the texts and print the adjectives\n",
    "for doc in nlp.pipe(novel):\n",
    "    print([token.text for token in doc])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env803",
   "language": "python",
   "name": "env803"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
