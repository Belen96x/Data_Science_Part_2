{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08a Exercises: Spacy & regular expressions\n",
    "\n",
    "In this exercise, we will practice writing some regex search patterns, explore the Spacy Matcher engine, and write custom pipeline Spacy components and understand how to add attributes to Spacy Doc objects (applicable to Span and Token objects too). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries: load modules and a spacy model\n",
    "We will use the `en_core_web_lg` model for better performance POS-tagging and named entity recognition performance. However, if this takes too long to run, switch to the `en_core_web_sm` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "from pprint import pprint\n",
    "from spacy import displacy\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# There are two ways to load a model\n",
    "# 1. use spacy.load\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# 2. import as a module \n",
    "# import en_core_web_lg\n",
    "# nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART A: Identify all the important characters in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: open and read the `emma-austen.txt` file as a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emma-austen.txt', encoding  = 'utf-8') as f: \n",
    "    lines = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Finding all mentions of Jane Fairfax in the file\n",
    "Write a regex search that will return the start index position of **all** the mentions of \"Jane Fairfax\" in the text. Note that she may be referred to as \"Miss Fairfax\" as well. Hint: use `finditer` and mix of lookahead/operators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ',\n",
       " 'Jane Fairfax ',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " ' Miss Fairfax',\n",
       " 'Jane Fairfax ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall('Jane Fairfax | Miss Fairfax', lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: identify the set of main characters in the book \n",
    "We will leverage Spacy's EntityRecognizer (https://spacy.io/api/entityrecognizer in preloaded (cf. `nlp.pipe_names`)) and use two rules of thumb to identify the main characters: (i) entities tagged as persons; and (ii) entities whose spans have more than 2 words. e.g. Emma Woodhouse, i.e. assume the author will give the full name of important characters as they are introduced.  \n",
    "\n",
    "First process with spacy (apply the `nlp` object to the string containing the text). **Make sure to save the Spacy Doc object to the variable `doc_emma`.** Use the `.ents` attribute to get the detected entities. Then use the `.label_` attributes to identify the *PERSON* entities; finally use the `.text` attribute to recover the str form of the span. \n",
    "\n",
    "Store the names (string form) of the main characters in a set with the variable name `main_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_emma = nlp(lines) #Store the document processed with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2178c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_characters_raw = [ent.text for ent in doc_emma.ents if ent.label_ == \"PERSON\" and len(ent.text.split()) == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "439fa04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Jane Austen', 'Emma Woodhouse', 'Miss Woodhouse', 'Farmer Mitchell’s', 'Frank Churchill', 'Miss Bates', 'Frank\\nChurchill', 'Donwell Abbey', 'Miss Smith', 'Harriet Smith', 'Harriet\\nSmith', 'Harriet Smith’s', 'Miss Prince', 'Miss Richardson', 'Robert Martin', 'Harriet good', 'John Knightley', 'Harriet, Harriet', 'Knightley.—“Robert Martin', 'CHAPTER X', 'Jane Fairfax', 'John\\nKnightley', 'Jane\\nFairfax', 'John Knightley.—“It', 'CHAPTER XIII', 'Harriet\\nseemed', 'her;—William Coxe', 'William Coxe', 'Harriet bore', 'Miss\\nWoodhouse', 'Miss\\nBates', 'Jane Bates', 'Miss Hawkins,—I', 'Miss Hawkins', 'Harriet: Harriet', 'Elizabeth Martin', 'Miss Woodhouse?)—for', 'Augusta Hawkins', 'Elizabeth\\nMartin', 'Philip Elton', 'York Tan', 'well:—a\\nman', 'Henry supplanted.—Mr', 'near—“Miss Bates', 'Jane Fairfax’s', 'Harriet rather', 'Anne Cox', 'William Cox', 'Miss Smith?—Very', 'John Saunders', 'William Larkins', 'Miss\\nSmith', 'Miss Bates?—I', 'Harriet earnestly', 'Clara Partridge', 'James Cooper', 'Jane Fairfax.—And', 'Jane Fairfax!”—thought', 'Emma Woodhouse-ing', 'CHAPTER XVI', 'E. The', 'Weston?—To Bath', 'Frank Churchill.—At', 'John\\nKnightley)—your', 'Uncle Knightley', 'Miss Smith!—Very', 'George Otway', 'Miss Bickerton', 'Harriet indignantly.—“Oh!', 'Harriet exultingly', 'Frank Churchill’s', 'Lady\\nPatroness', 'Donwell Lane', 'story.—Robert Martin', 'Abbey fish-ponds', 'E., Knightley', 'John Abdy', 'John ostler', 'Jane\\nFairfax’s.—Of', 'jealousy.—In Jane’s', 'Harriet\\nagain', 'Harriet Smith!—It', 'Harriet Smith!—Such', 'Frank Churchill.—I', 'Frank Churchill.—He', 'Jane Fairfax.—Then', 'low.—Frank Churchill', 'Miss F', 'Miss W.', 'Miss F.', 'me.—In short', 'F. C.', 'Miss Bates.—He', 'Anna Weston', 'Robert\\nMartin', 'William\\nLarkins', 'herself.—Robert Martin', 'Project Gutenberg', 'Project Gutenberg-tm', 'Project Gutenberg-', \"Project Gutenberg-tm's\"]\n"
     ]
    }
   ],
   "source": [
    "main_characters = [] #It's weird why its retrieving CHAPTER X as person\n",
    "\n",
    "for char in main_characters_raw:\n",
    "    if char not in main_characters:\n",
    "        main_characters.append(char)\n",
    "\n",
    "print(main_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Get a sense of how each character is protrayed in the text\n",
    "We will leverage the rules-based matching (using the Matcher https://spacy.io/usage/rule-based-matching) \n",
    "A pattern we can use is to look for spans that are tagged `PERSON` and look for adjectives surrounding them. For e.g. (1) \"*beautiful* Emma\" or (2) \"Emma Woodhouse is *charming*\".\n",
    "\n",
    "The pattern for (1) has been done for you -- we look for spans with 1 or more tokens (using the [quantifier](https://spacy.io/usage/rule-based-matching#quantifiers) \"OP\": \"{1,}\") that have been labeled as `PERSON` as well as the token preceding it that has a ADJ part-of-speech tag. **Your task is to do the same for (2) and add it to the matcher and then run matcher on `doc_emma`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"POS\": \"ADJ\"}, \n",
    "            {\"ENT_TYPE\": \"PERSON\", \"OP\": \"{1,}\"}]\n",
    "pattern2 = [{\"ENT_TYPE\": \"PERSON\", \"OP\": \"{1,}\"}, {\"LEMMA\": \"be\", \"POS\": \"VERB\"}, {\"POS\": \"ADJ\"}] #You stablish here the patterns \n",
    "matcher.add('Patterns added',[pattern1, pattern2]) #Then add them to the matcher. This can work with any type of pattern! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc_emma) #You store the matches to the doc that you have. This is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "028867e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poor James\n",
      "poor Isabella\n",
      "Dear Emma\n",
      "little Frank\n",
      "dear Emma\n",
      "little Perrys\n",
      "little Welch\n",
      "bad thing?—why\n",
      "dear Emma\n",
      "little George\n",
      "dear Harriet\n",
      "dearest Harriet\n",
      "dear Harriet\n",
      "dear Harriet\n",
      "dear Harriet\n",
      "dear Harriet\n",
      "poor Isabella\n",
      "poor Isabella\n",
      "poor Isabella\n",
      "poor Isabella\n",
      "dear Emma\n",
      "dear Emma\n",
      "Little Emma\n",
      "dear Isabella\n",
      "dear Emma\n",
      "little Bella\n",
      "little Bella\n",
      "good Bateses\n",
      "amiable Jane\n",
      "amiable Jane Fairfax\n",
      "dear Emma\n",
      "ill\n",
      "\n",
      "ill\n",
      "behind!—Most\n",
      "dear Emma\n",
      "dear Isabella\n",
      "poor Isabella\n",
      "poor Emma\n",
      "poor Harriet\n",
      "young Martin\n",
      "poor Harriet\n",
      "poor Isabella\n",
      "dear Emma\n",
      "poor Jane\n",
      "dear Emma\n",
      "dear Emma\n",
      "dear Jane\n",
      "dear Jane\n",
      "poor Harriet\n",
      "fortunate Miss\n",
      "fortunate Miss Hawkins\n",
      "charming Augusta\n",
      "charming Augusta Hawkins\n",
      "dear Emma\n",
      "dear Emma\n",
      "dear Emma\n",
      "poor Jane\n",
      "poor Jane Fairfax\n",
      "dear Emma\n",
      "little Henry\n",
      "dear Emma\n",
      "little Henry\n",
      "dear Emma\n",
      "dear Emma\n",
      "Little Henry\n",
      "perfect Jane\n",
      "perfect Jane Fairfax\n",
      "young Cox\n",
      "young Cox\n",
      "little Harriet\n",
      "good exchange?—You\n",
      "little Emma\n",
      "poor Isabella\n",
      "dear Emma\n",
      "foolish preparation!—You\n",
      "dear William\n",
      "dear William Larkins\n",
      "last Emma\n",
      "poor Harriet\n",
      "poor Harriet\n",
      "sweet Jane\n",
      "sweet Jane Fairfax\n",
      "dear Emma\n",
      "dear Emma\n",
      "little Harriet\n",
      "dear Jane\n",
      "dear Jane\n",
      "dear Jane\n",
      "dear Jane\n",
      "dear Jane\n",
      "certain Frank\n",
      "certain Frank Churchill\n",
      "little Henry\n",
      "little John\n",
      "dear Emma\n",
      "comfortable carriage.—Oh\n",
      "dear Jane\n",
      "little Miss\n",
      "little Miss Smith!—Very\n",
      "dear Jane\n",
      "dear Jane\n",
      "poor Harriet\n",
      "dearest Harriet\n",
      "dear Harriet\n",
      "dear Emma\n",
      "old Abbey\n",
      "old Abbey fish\n",
      "old Abbey fish-\n",
      "old Abbey fish-ponds\n",
      "afraid Jane\n",
      "little Sucklings\n",
      "old John\n",
      "old John Abdy\n",
      "dear Jane\n",
      "Dear Emma\n",
      "dear Jane\n",
      "dear Jane\n",
      "poor Frank\n",
      "dear Emma\n",
      "dear Emma\n",
      "dear Emma\n",
      "poor Harriet\n",
      "dear Harriet\n",
      "decisive dances.—Emma\n",
      "dearest Emma\n",
      "low.—Frank Churchill\n",
      "own Emma\n",
      "dear Emma\n",
      "affectionate Son\n",
      "dear Emma\n",
      "poor Harriet\n",
      "poor Harriet\n",
      "poor Jane\n",
      "dearest Emma\n",
      "understanding:—Miss Taylor\n",
      "little Anna\n",
      "little Anna Weston\n",
      "dear Emma\n",
      "poor Emma\n",
      "dearest Emma\n",
      "dear Emma\n",
      "little John\n",
      "poor Martin\n",
      "own Jane\n",
      "most Project\n",
      "most Project Gutenberg\n",
      "most Project Gutenberg-\n",
      "most Project Gutenberg-tm\n",
      "full Project\n",
      "full Project Gutenberg\n",
      "full Project Gutenberg-\n",
      "full Project Gutenberg-tm\n",
      "other Project\n",
      "other Project Gutenberg\n",
      "other Project Gutenberg-\n",
      "other Project Gutenberg-tm\n",
      "full Project\n",
      "full Project Gutenberg\n",
      "full Project Gutenberg-\n",
      "full Project Gutenberg-tm\n",
      "individual Project\n",
      "individual Project Gutenberg\n",
      "individual Project Gutenberg-\n",
      "individual Project Gutenberg-tm\n",
      "individual Project\n",
      "individual Project Gutenberg\n",
      "individual Project Gutenberg-\n",
      "individual Project Gutenberg-tm\n",
      "full Project\n",
      "full Project Gutenberg\n",
      "full Project Gutenberg-\n",
      "full Project Gutenberg-tm\n",
      "official Project\n",
      "official Project Gutenberg\n",
      "official Project Gutenberg-\n",
      "official Project Gutenberg-tm\n",
      "full Project\n",
      "full Project Gutenberg\n",
      "full Project Gutenberg-\n",
      "full Project Gutenberg-tm\n",
      "full Project\n",
      "full Project Gutenberg\n",
      "full Project Gutenberg-\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in matches:\n",
    "    span = doc_emma[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5 (provided): Run matcher on the document \n",
    "**Note:** we set the `as_spans` parameter as True so the results will be returned as Spacy.Span objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E084] Error assigning label ID 8749331614196081296 to span: not in StringStore.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 17\u001b[0m\n\u001b[0;32m     14\u001b[0m     _ \u001b[38;5;241m=\u001b[39m nlp\u001b[38;5;241m.\u001b[39mvocab\u001b[38;5;241m.\u001b[39mstrings[label]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Step 3: Run the matcher again with as_spans=True\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m matches \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_emma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_spans\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Collect main character descriptions\u001b[39;00m\n\u001b[0;32m     20\u001b[0m main_char_desc \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\belen\\anaconda3\\Lib\\site-packages\\spacy\\matcher\\matcher.pyx:311\u001b[0m, in \u001b[0;36mspacy.matcher.matcher.Matcher.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\belen\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\span.pyx:111\u001b[0m, in \u001b[0;36mspacy.tokens.span.Span.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E084] Error assigning label ID 8749331614196081296 to span: not in StringStore."
     ]
    }
   ],
   "source": [
    "#There is a problem here. I need to put the labels, because without them it doesn't really work\n",
    "#To solve it, you need to follow this steps:\n",
    "\n",
    "# Step 1: Run the matcher to get the matches without as_spans=True\n",
    "matches = matcher(doc_emma)\n",
    "\n",
    "# Step 2: Extract unique labels from the matches and register them\n",
    "unique_labels = set()\n",
    "for match_id, start, end in matches:\n",
    "    unique_labels.add(nlp.vocab.strings[match_id])\n",
    "\n",
    "# Register unique labels in the StringStore\n",
    "for label in unique_labels:\n",
    "    _ = nlp.vocab.strings[label]\n",
    "\n",
    "# Step 3: Run the matcher again with as_spans=True\n",
    "matches = matcher(doc_emma, as_spans=True)\n",
    "\n",
    "# Collect main character descriptions\n",
    "main_char_desc = []\n",
    "for span in matches:\n",
    "    main_char_desc.append((span.label_, span.text))\n",
    "\n",
    "# Print the main character descriptions\n",
    "print(main_char_desc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6: add two custom pipeline components\n",
    "Create two new methods called `identify_main_characters` and `characters_descs` and move your code for Exercise 2 and 3 into each of them. The objective is to be able to add these custom components that will also be applied when calling `nlp` on a text. \n",
    "\n",
    "Each method should extend the attributes for the Doc object. \n",
    "- For `identify_main_characters`, a new `main_characters` attribute should be added and it should hold the set of the found main character names (in str form) after processing is done. \n",
    "- For `characters_descs`, a new `characters_descriptions` attribute should be added. This should hold the set of adjective+character names found (in str form). \n",
    "\n",
    "Refer to `08a_spacy.ipynb` and look for how to add **custom components** and **extension attributes**. Look also at the spacy documentation on (1) [Creating custom pipeline components](https://spacy.io/usage/processing-pipelines#custom-components) and (2) [Extension attributes ](https://spacy.io/usage/processing-pipelines#custom-components-attributes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a463c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Characters: set()\n",
      "Character Descriptions: set()\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.language import Language\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7 (provided): add the custom components and run processing on the text again\n",
    "Inspect the outputs of the two custom components to see the main characters in the text as well as the an idea of how the characters are protrayed in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'identify_main_characters', 'characters_descs']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "[E046] Can't retrieve unregistered extension attribute 'identify_main_characters'. Did you forget to call the `set_extension` method?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(nlp\u001b[38;5;241m.\u001b[39mpipe_names)\n\u001b[0;32m      3\u001b[0m doc_emma_new  \u001b[38;5;241m=\u001b[39m nlp(lines)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mdoc_emma_new\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midentify_main_characters\u001b[49m, doc_emma_new\u001b[38;5;241m.\u001b[39m_\u001b[38;5;241m.\u001b[39mcharacters_descriptions\n",
      "File \u001b[1;32mc:\\Users\\belen\\anaconda3\\Lib\\site-packages\\spacy\\tokens\\underscore.py:48\u001b[0m, in \u001b[0;36mUnderscore.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions:\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE046\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m     49\u001b[0m     default, method, getter, setter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extensions[name]\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m getter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: [E046] Can't retrieve unregistered extension attribute 'identify_main_characters'. Did you forget to call the `set_extension` method?"
     ]
    }
   ],
   "source": [
    "# Add the component to the pipeline\n",
    "print(nlp.pipe_names)\n",
    "doc_emma_new  = nlp(lines)\n",
    "doc_emma_new._.identify_main_characters, doc_emma_new._.characters_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Write a function that extracts the text in each chapter of the book\n",
    "The content layout of the book is as follows: \n",
    "\n",
    "**Contents**\n",
    "\n",
    " VOLUME I.\n",
    " CHAPTER I.\n",
    " CHAPTER II.\n",
    " CHAPTER III.\n",
    " CHAPTER IV.\n",
    " CHAPTER V.\n",
    " CHAPTER VI.\n",
    " CHAPTER VII.\n",
    " CHAPTER VIII.\n",
    " CHAPTER IX.\n",
    " CHAPTER X.\n",
    " CHAPTER XI.\n",
    " CHAPTER XII.\n",
    " CHAPTER XIII.\n",
    " CHAPTER XIV.\n",
    " CHAPTER XV.\n",
    " CHAPTER XVI.\n",
    " CHAPTER XVII.\n",
    " CHAPTER XVIII.\n",
    "\n",
    " VOLUME II.\n",
    " CHAPTER I.\n",
    " CHAPTER II.\n",
    " CHAPTER III.\n",
    " CHAPTER IV.\n",
    " CHAPTER V.\n",
    " CHAPTER VI.\n",
    " CHAPTER VII.\n",
    " CHAPTER VIII.\n",
    " CHAPTER IX.\n",
    " CHAPTER X.\n",
    " CHAPTER XI.\n",
    " CHAPTER XII.\n",
    " CHAPTER XIII.\n",
    " CHAPTER XIV.\n",
    " CHAPTER XV.\n",
    " CHAPTER XVI.\n",
    " CHAPTER XVII.\n",
    " CHAPTER XVIII.\n",
    "\n",
    " VOLUME III.\n",
    " CHAPTER I.\n",
    " CHAPTER II.\n",
    " CHAPTER III.\n",
    " CHAPTER IV.\n",
    " CHAPTER V.\n",
    " CHAPTER VI.\n",
    " CHAPTER VII.\n",
    " CHAPTER VIII.\n",
    " CHAPTER IX.\n",
    " CHAPTER X.\n",
    " CHAPTER XI.\n",
    " CHAPTER XII.\n",
    " CHAPTER XIII.\n",
    " CHAPTER XIV.\n",
    " CHAPTER XV.\n",
    " CHAPTER XVI.\n",
    " CHAPTER XVII.\n",
    " CHAPTER XVIII.\n",
    " CHAPTER XIX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8 (provided): open the file and read its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emma-austen.txt', encoding  = 'utf-8') as f: \n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9: compile two regex objects to identify lines for volume and chapter headers \n",
    "An initial set of solutions have been made for you, but they need to be corrected. You will need to correct them by (1) adding/changing some parts of them and (2) simplifying/removing duplicated patterns. Note: you should assume the following:\n",
    "- volume and chapter headers can be numbered in arabic or latin numerals (\"Volume 5\"/\"Volume V\" etc)\n",
    "- the headers can be title-cased, lowercased or capitalised (e.g. \"Volume\", \"volume\" or \"VOLUME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE ###\n",
    "import re\n",
    "r_volume = re.compile(r'\\bvolume\\b (\\d+|[ivxlcdm]+)', re.IGNORECASE)\n",
    "r_chapter = re.compile(r'\\bchapter\\b (\\d+|[ivxlcdm]+)', re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10: use the two regex object above in the following code snippet\n",
    "The goal is to populate dictionary which we will name `book`. Each volume of the text will have an entry in `book`, which in turn contains the chapters in the volume. Each chapter is a list of lines that follows the order of the text. **All of the keys in your dictionary must be strings.**\n",
    "\n",
    "NOTE: you need to identify two areas in the code snippet that need changes to meet the specifications above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE ###\n",
    "\n",
    "book = {}\n",
    "curr_vol = None\n",
    "curr_chap = None\n",
    "\n",
    "for l in lines:\n",
    "    vline = re.match(r_volume, l)\n",
    "    cline = re.match(r_chapter, l)\n",
    "    \n",
    "    if vline: \n",
    "        curr_vol = vline\n",
    "        if curr_vol not in book: \n",
    "            book[curr_vol] = {}\n",
    "            curr_chap = None\n",
    "        continue\n",
    "\n",
    "    elif cline:\n",
    "        curr_chap = cline\n",
    "        if curr_chap not in book[curr_vol]:\n",
    "            \n",
    "            book[curr_vol][curr_chap] = []\n",
    "    \n",
    "    elif  curr_chap != None and curr_vol != None:\n",
    "        book[curr_vol][curr_chap].append(l)\n",
    "\n",
    "for v in book:\n",
    "    print(f'{v}\\n\\n')\n",
    "    for c in book[v]:\n",
    "        print(f'{c}\\n{book[v][c][3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check: make sure your changes to the code snippet achieved the desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in book.items():\n",
    "    print(k, len(v))\n",
    "    for k2, v2 in v.items():\n",
    "        print('\\t\\t', k2, len(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Identifying the characters and protrayal information on a cleaner version of the text\n",
    "\n",
    "#### Exercise 11: Apply your custom spacy components on each chapter\n",
    "\n",
    "Collect the set of main characters and character descriptions from these. Compare it with your initial set applied to the contents of the entire .txt file. \n",
    "\n",
    "Apply `nlp` to the text associated with each chapter. Note: when defining the custom components and setting the new attributes (using `.set_extension`), the \"force = True\" parameter should be set; this allows the same `nlp` object to be reused and each time the added attributes can be reset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
