{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08a Exercises: Spacy & regular expressions\n",
    "\n",
    "In this exercise, we will practice writing some regex search patterns, explore the Spacy Matcher engine, and write custom pipeline Spacy components and understand how to add attributes to Spacy Doc objects (applicable to Span and Token objects too). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminaries: load modules and a spacy model\n",
    "We will use the `en_core_web_lg` model for better performance POS-tagging and named entity recognition performance. However, if this takes too long to run, switch to the `en_core_web_sm` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "from pprint import pprint\n",
    "from spacy import displacy\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# There are two ways to load a model\n",
    "# 1. use spacy.load\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "# 2. import as a module \n",
    "# import en_core_web_lg\n",
    "# nlp = en_core_web_lg.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART A: Identify all the important characters in the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 1: open and read the `emma-austen.txt` file as a single string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emma-austen.txt', encoding  = 'utf-8') as f: \n",
    "    lines = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 2: Finding all mentions of Jane Fairfax in the file\n",
    "Write a regex search that will return the start index position of **all** the mentions of \"Jane Fairfax\" in the text. Note that she may be referred to as \"Miss Fairfax\" as well. Hint: use `finditer` and mix of lookahead/operators. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 3: identify the set of main characters in the book \n",
    "We will leverage Spacy's EntityRecognizer (https://spacy.io/api/entityrecognizer in preloaded (cf. `nlp.pipe_names`)) and use two rules of thumb to identify the main characters: (i) entities tagged as persons; and (ii) entities whose spans have more than 2 words. e.g. Emma Woodhouse, i.e. assume the author will give the full name of important characters as they are introduced.  \n",
    "\n",
    "First process with spacy (apply the `nlp` object to the string containing the text). **Make sure to save the Spacy Doc object to the variable `doc_emma`.** Use the `.ents` attribute to get the detected entities. Then use the `.label_` attributes to identify the *PERSON* entities; finally use the `.text` attribute to recover the str form of the span. \n",
    "\n",
    "Store the names (string form) of the main characters in a set with the variable name `main_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4: Get a sense of how each character is protrayed in the text\n",
    "We will leverage the rules-based matching (using the Matcher https://spacy.io/usage/rule-based-matching) \n",
    "A pattern we can use is to look for spans that are tagged `PERSON` and look for adjectives surrounding them. For e.g. (1) \"*beautiful* Emma\" or (2) \"Emma Woodhouse is *charming*\".\n",
    "\n",
    "The pattern for (1) has been done for you -- we look for spans with 1 or more tokens (using the [quantifier](https://spacy.io/usage/rule-based-matching#quantifiers) \"OP\": \"{1,}\") that have been labeled as `PERSON` as well as the token preceding it that has a ADJ part-of-speech tag. **Your task is to do the same for (2) and add it to the matcher and then run matcher on `doc_emma`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"POS\": \"ADJ\"}, \n",
    "            {\"ENT_TYPE\": \"PERSON\", \"OP\": \"{1,}\"}]\n",
    "matcher.add(\"Main char description #1\", [pattern1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 5 (provided): Run matcher on the document \n",
    "**Note:** we set the `as_spans` parameter as True so the results will be returned as Spacy.Span objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc_emma, as_spans = True)\n",
    "\n",
    "main_char_desc = set()\n",
    "for span in matches:\n",
    "    main_char_desc.add((span.label_, span.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 6: add two custom pipeline components\n",
    "Create two new methods called `identify_main_characters` and `characters_descs` and move your code for Exercise 2 and 3 into each of them. The objective is to be able to add these custom components that will also be applied when calling `nlp` on a text. \n",
    "\n",
    "Each method should extend the attributes for the Doc object. \n",
    "- For `identify_main_characters`, a new `main_characters` attribute should be added and it should hold the set of the found main character names (in str form) after processing is done. \n",
    "- For `characters_descs`, a new `characters_descriptions` attribute should be added. This should hold the set of adjective+character names found (in str form). \n",
    "\n",
    "Refer to `08a_spacy.ipynb` and look for how to add **custom components** and **extension attributes**. Look also at the spacy documentation on (1) [Creating custom pipeline components](https://spacy.io/usage/processing-pipelines#custom-components) and (2) [Extension attributes ](https://spacy.io/usage/processing-pipelines#custom-components-attributes). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE ###\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"identify_main_characters\")\n",
    "def identify_main_characters(doc):\n",
    "    '''\n",
    "    (doc string here)\n",
    "    '''\n",
    "    \n",
    "    return doc\n",
    "\n",
    "@Language.component(\"characters_descs\")\n",
    "def characters_descs(doc):\n",
    "    '''\n",
    "    (doc string here)\n",
    "    '''\n",
    "    \n",
    "    return doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 7 (provided): add the custom components and run processing on the text again\n",
    "Inspect the outputs of the two custom components to see the main characters in the text as well as the an idea of how the characters are protrayed in it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"identify_main_characters\")\n",
    "nlp.add_pipe(\"characters_descs\")\n",
    "print(nlp.pipe_names)\n",
    "doc_emma_new  = nlp(lines)\n",
    "doc_emma_new._.main_characters, doc_emma_new._.characters_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Write a function that extracts the text in each chapter of the book\n",
    "The content layout of the book is as follows: \n",
    "\n",
    "**Contents**\n",
    "\n",
    " VOLUME I.\n",
    " CHAPTER I.\n",
    " CHAPTER II.\n",
    " CHAPTER III.\n",
    " CHAPTER IV.\n",
    " CHAPTER V.\n",
    " CHAPTER VI.\n",
    " CHAPTER VII.\n",
    " CHAPTER VIII.\n",
    " CHAPTER IX.\n",
    " CHAPTER X.\n",
    " CHAPTER XI.\n",
    " CHAPTER XII.\n",
    " CHAPTER XIII.\n",
    " CHAPTER XIV.\n",
    " CHAPTER XV.\n",
    " CHAPTER XVI.\n",
    " CHAPTER XVII.\n",
    " CHAPTER XVIII.\n",
    "\n",
    " VOLUME II.\n",
    " CHAPTER I.\n",
    " CHAPTER II.\n",
    " CHAPTER III.\n",
    " CHAPTER IV.\n",
    " CHAPTER V.\n",
    " CHAPTER VI.\n",
    " CHAPTER VII.\n",
    " CHAPTER VIII.\n",
    " CHAPTER IX.\n",
    " CHAPTER X.\n",
    " CHAPTER XI.\n",
    " CHAPTER XII.\n",
    " CHAPTER XIII.\n",
    " CHAPTER XIV.\n",
    " CHAPTER XV.\n",
    " CHAPTER XVI.\n",
    " CHAPTER XVII.\n",
    " CHAPTER XVIII.\n",
    "\n",
    " VOLUME III.\n",
    " CHAPTER I.\n",
    " CHAPTER II.\n",
    " CHAPTER III.\n",
    " CHAPTER IV.\n",
    " CHAPTER V.\n",
    " CHAPTER VI.\n",
    " CHAPTER VII.\n",
    " CHAPTER VIII.\n",
    " CHAPTER IX.\n",
    " CHAPTER X.\n",
    " CHAPTER XI.\n",
    " CHAPTER XII.\n",
    " CHAPTER XIII.\n",
    " CHAPTER XIV.\n",
    " CHAPTER XV.\n",
    " CHAPTER XVI.\n",
    " CHAPTER XVII.\n",
    " CHAPTER XVIII.\n",
    " CHAPTER XIX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 8 (provided): open the file and read its contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emma-austen.txt', encoding  = 'utf-8') as f: \n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 9: compile two regex objects to identify lines for volume and chapter headers \n",
    "An initial set of solutions have been made for you, but they need to be corrected. You will need to correct them by (1) adding/changing some parts of them and (2) simplifying/removing duplicated patterns. Note: you should assume the following:\n",
    "- volume and chapter headers can be numbered in arabic or latin numerals (\"Volume 5\"/\"Volume V\" etc)\n",
    "- the headers can be title-cased, lowercased or capitalised (e.g. \"Volume\", \"volume\" or \"VOLUME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE ###\n",
    "import re\n",
    "r_volume  = re.compile(r'VOLUME (\\d+|[0-9]+|[ixv]+)\\b')\n",
    "r_chapter = re.compile(r'ChAPTER (\\d+|[ixv]+|[IXV]+)\\b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 10: use the two regex object above in the following code snippet\n",
    "The goal is to populate dictionary which we will name `book`. Each volume of the text will have an entry in `book`, which in turn contains the chapters in the volume. Each chapter is a list of lines that follows the order of the text. **All of the keys in your dictionary must be strings.**\n",
    "\n",
    "NOTE: you need to identify two areas in the code snippet that need changes to meet the specifications above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE ###\n",
    "\n",
    "book = {}\n",
    "curr_vol = None\n",
    "curr_chap = None\n",
    "\n",
    "for l in lines:\n",
    "    vline = re.match(r_volume, l)\n",
    "    cline = re.match(r_chapter, l)\n",
    "    \n",
    "    if vline: \n",
    "        curr_vol = vline\n",
    "        if curr_vol not in book: \n",
    "            book[curr_vol] = {}\n",
    "            curr_chap = None\n",
    "        continue\n",
    "\n",
    "    elif cline:\n",
    "        curr_chap = cline\n",
    "        if curr_chap not in book[curr_vol]:\n",
    "            \n",
    "            book[curr_vol][curr_chap] = []\n",
    "    \n",
    "    elif  curr_chap != None and curr_vol != None:\n",
    "        book[curr_vol][curr_chap].append(l)\n",
    "\n",
    "for v in book:\n",
    "    print(f'{v}\\n\\n')\n",
    "    for c in book[v]:\n",
    "        print(f'{c}\\n{book[v][c][3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sanity check: make sure your changes to the code snippet achieved the desired output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in book.items():\n",
    "    print(k, len(v))\n",
    "    for k2, v2 in v.items():\n",
    "        print('\\t\\t', k2, len(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Identifying the characters and protrayal information on a cleaner version of the text\n",
    "\n",
    "#### Exercise 11: Apply your custom spacy components on each chapter\n",
    "\n",
    "Collect the set of main characters and character descriptions from these. Compare it with your initial set applied to the contents of the entire .txt file. \n",
    "\n",
    "Apply `nlp` to the text associated with each chapter. Note: when defining the custom components and setting the new attributes (using `.set_extension`), the \"force = True\" parameter should be set; this allows the same `nlp` object to be reused and each time the added attributes can be reset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR SOLUTION HERE ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
